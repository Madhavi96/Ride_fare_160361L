# -*- coding: utf-8 -*-
"""ride_fare_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z2jxxpfZ809HjgSrQkEJwwX0hZyx4wBg
"""

import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import StandardScaler,MinMaxScaler
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Load libraries
from pandas import read_csv
from pandas.plotting import scatter_matrix
from matplotlib import pyplot

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

X_train = pd.read_csv('/content/drive/My Drive/Ride_Fare/train.csv',index_col='tripid',)

X_train

X_train.describe()

'''
num_rows=X_train.shape[0]
duration_list = []
for i in range(num_rows):
  duration = (datetime.strptime(X_train.iloc[i]['drop_time'],'%m/%d/%Y %H:%M')- datetime.strptime(X_train.iloc[i]['pickup_time'],'%m/%d/%Y %H:%M')).seconds
  duration_list.append(duration)

X_train['duration'] = duration_list
'''

X_train.drop('drop_time',axis = 1,inplace=True)
X_train.drop('pickup_time',axis = 1,inplace=True)

X_train.corr()

'''
num_rows=X_train.shape[0]
waiting_list = []
for i in range(num_rows):
  waiting = (X_train.iloc[i]['meter_waiting'])+ (X_train.iloc[i]['meter_waiting_till_pickup'])
  waiting_list.append(waiting)

X_train['waiting'] = waiting_list
'''

'''
X_train.drop('meter_waiting',axis = 1,inplace=True)
X_train.drop('meter_waiting_till_pickup',axis = 1,inplace=True)

#X_train.drop('meter_waiting_till_pickup',axis = 1,inplace=True)
'''

####### distances
num_rows=X_train.shape[0]
distance_list = []

for i in range(num_rows):
  longitude = np.abs(X_train.iloc[i]['pick_lon']  -  X_train.iloc[i]['drop_lon'])
  lattitude = np.abs(X_train.iloc[i]['pick_lat'] -  X_train.iloc[i]['drop_lat'])
  dist = np.sqrt((longitude)**2+(lattitude)**2)
  distance_list.append(dist)

X_train['distance'] = distance_list

X_train.drop('pick_lat',axis = 1,inplace=True)
X_train.drop('pick_lon',axis = 1,inplace=True)
X_train.drop('drop_lat',axis = 1,inplace=True)
X_train.drop('drop_lon',axis = 1,inplace=True)

X_train

X_train.describe()

########################## outliers

##################### distance

plt.scatter(X_train['distance'],X_train['fare'])

outlier_dist_row = X_train.loc[X_train['distance']>50].index
outlier_dist_row

X_train.drop(outlier_dist_row,inplace=True)

########### meter_waiting_till_pickup

plt.scatter(X_train['meter_waiting_till_pickup'],X_train['fare'])

outlier_waiting_till_pickup_row = X_train.loc[X_train['meter_waiting_till_pickup']>6000]
outlier_waiting_till_pickup_row

#ids = X_train.index[['191056435','193827380','205639292']]
X_train.drop(191056435,inplace=True)
X_train.drop(193827380,inplace=True)
X_train.drop(205639292,inplace=True)

###############   meter_waiting_fare



plt.scatter(X_train['meter_waiting_fare'],X_train['fare'])

outlier_waiting_time = X_train.loc[X_train['fare']>15000]
outlier_waiting_time

############### All looks fine above

num_rows=X_train.shape[0]
li =[]

for i in range(num_rows):
  if X_train.iloc[i]['meter_waiting'] >  X_train.iloc[i]['duration']:
    li.append(i)
li

X_train.iloc[[11046, 12683, 12894, 14039]]

X_train.drop(207947602,inplace=True)













X_train

X_train['label'] = X_train['label'].replace({'correct':1 , 'incorrect':0})

X_train = X_train.fillna(X_train.mean())

def split_train_val(X_train):
  X_train_0 = X_train.loc[X_train['label'] == 0]
  X_train_1 = X_train.loc[X_train['label'] == 1]



  val_0 = X_train_0.sample(n=250,random_state=100)
  train_0 = X_train_0.drop(val_0.index)

  val_1 = X_train_1.sample(n=250,random_state=100)
  train_1 = X_train_1.drop(val_1.index)

  validation_set = pd.concat([val_0,val_1],axis=0)
  validation_set = validation_set.sample(frac=1)

  train_set = pd.concat([train_0,train_1],axis=0)
  train_set = train_set.sample(frac=1)

  train_set_y = train_set['label']
  train_set_x = train_set.drop('label', axis=1)

  validation_set_y = validation_set['label']
  validation_set_x = validation_set.drop('label', axis=1)

  return train_set_x, train_set_y,validation_set_x, validation_set_y

train_set_x, train_set_y,validation_set_x, validation_set_y = split_train_val(X_train)

scaler = MinMaxScaler()
scaler.fit(train_set_x)

scaled_train_x = scaler.transform(train_set_x)
scaled_validation_x = scaler.transform(validation_set_x)

weights = {0:15,1:1}

from keras.models import Sequential
from keras.layers import Dense,Dropout

# define the keras model
model = Sequential()
model.add(Dense(256, input_dim=scaled_train_x.shape[1], activation='relu'))

model.add(Dense(128, activation='relu'))

model.add(Dense(64, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# compile the keras model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

weights = {0:2,1:1}

# fit the keras model on the 24
history = model.fit(scaled_train_x, train_set_y, epochs=80, batch_size=15,validation_split=0.2,class_weight=weights)



print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

preds = model.predict(scaled_validation_x )

class_preds=[]
for pred in preds:
  if pred>0.5:
    class_preds.append(1)
  else:
    class_preds.append(0)

from sklearn.metrics import f1_score

f1 = f1_score(validation_set_y, class_preds, average='macro')
f1

confusion_mat = confusion_matrix(validation_set_y, class_preds)
confusion_mat

import pickle
f = open('/content/drive/My Drive/Ride_Fare/model_3_1_weight_155z_242o','wb')
pickle.dump(model,f)

############# better
import pickle
f = open('/content/drive/My Drive/Ride_Fare/model_2_1_weight_160z_243o','wb')
pickle.dump(model,f)

############# better
import pickle
f = open('/content/drive/My Drive/Ride_Fare/model_2_1_weight_178z_243o','wb')
pickle.dump(model,f)

################ open pickle file
with open('/content/drive/My Drive/Ride_Fare/model_2_1_weight_160z_243o', "rb") as f:
    myobj = pickle.load(f)



###################################################################################################



test_df = pd.read_csv('/content/drive/My Drive/Ride_Fare/test.csv',index_col="tripid")

test_df = test_df.drop('drop_time',axis = 1)
test_df = test_df.drop('pickup_time',axis = 1)



####### distances
num_rows=test_df.shape[0]
distance_list = []

for i in range(num_rows):
  longitude = np.abs(test_df.iloc[i]['pick_lon']  -  test_df.iloc[i]['drop_lon'])
  lattitude = np.abs(test_df.iloc[i]['pick_lat'] -  test_df.iloc[i]['drop_lat'])
  dist = np.sqrt((longitude)**2+(lattitude)**2)
  distance_list.append(dist)

test_df['distance'] = distance_list

test_df.drop('pick_lat',axis = 1,inplace=True)
test_df.drop('pick_lon',axis = 1,inplace=True)
test_df.drop('drop_lat',axis = 1,inplace=True)
test_df.drop('drop_lon',axis = 1,inplace=True)

X = X_train.drop('label',axis=1)

X.mean()

test_df = test_df.fillna(X.mean())

test_df = scaler.transform(test_df)

################################3 predict from model

### NN
test_preds = model.predict(test_df)

test_preds

submission_df = pd.read_csv('/content/drive/My Drive/Ride_Fare/sample_submission.csv',index_col="tripid")
submission_df.head()

class_preds=[]
for pred in test_preds:
  if pred>0.5:
    class_preds.append(1)
  else:
    class_preds.append(0)

# Make sure we have the rows in the same order


# Save predictions to submission data frame
submission_df["prediction"] = class_preds

submission_df.head()

# 1_1 clas wight, H= all data, L = part
submission_df.to_csv('/content/drive/My Drive/Ride_Fare/ride_fare_submission_2_1_weight_178z_243o.csv', index=True)

submission_df['prediction'].value_counts()





